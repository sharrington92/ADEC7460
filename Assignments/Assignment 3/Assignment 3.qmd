---
title: "Assignment 3"
author: "Shaun Harrington"
format: 
  pdf: default
  html:
    toc: true
    code-fold: true
editor_options: 
  chunk_output_type: console
---

<!-- Pick any monthly time series you choose with at least five years of data that you have yet to use previously.  -->

<!-- Build 1 ETS, 1 ARIMA, 1 NNETAR, and one ensemble model on four years of that data.  Forecast the 5th year.  Provide metrics to assess performance. -->

## Introduction

I'll be using data from the EIA to predict wind generation in Texas using prices. Theory would indicate that higher prices will incentivize more electricity production. Because wind turbines are relatively cheap to build and Texas' unique electricity market enables a wide number of suppliers to join the market, it would make sense for prices to be a predictor of wind turbine generation.

One source of volatility that could diminish this relation would be the variation of wind. I was unable to find wind data for Texas, but an alternative data series that could fit is capacity factor. This is defined as $\frac{Average MWh}{Total Capacity}$. When prices are positive (and above marginal cost $\approx$ 0), wind turbines will generate as long as there is wind. Thus while a drop in capacity factor could be due to negative pricing, it is much more likely to be from a decrease in wind. 


```{r}
#| warning: false

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(tidyverse)
library(fpp3)
library(fredr)
library(scales)
library(jsonlite)
library(zoo)

theme_set(theme_bw())

if(!str_detect(basename(getwd()), "Time Series") & str_detect(dirname(getwd()), "Time Series")){
  repeat{
    setwd("../")
    if(str_detect(basename(getwd()), "Time Series")){
      break
    }
  }
}

if(basename(getwd()) != "Assignment 2") setwd(file.path(getwd(), "Assignments", "Assignment 2"))



```

### Get Data


```{r}

 eia.key <- Sys.getenv("EIA_API_KEY")
  
    
  fn_query_eia <- function( api_url = NULL,
    the_series_id, the_source = "steo", the_frequency = "monthly", the_facet = "seriesId",
    the_offset = 0, the_length = 5000, the_eia_key = eia.key){
    
    if(is.null(api_url)){
      the_url = "https://api.eia.gov/v2/"
    
      # Query must be no more than 5,000
      if(the_length > 5000) break
      
      get_call <- paste0(the_url, the_source, "/data/?", paste(
        paste0("frequency=", the_frequency), 
        "data[0]=value", 
        paste0("facets[", the_facet, "][]=", the_series_id), 
        "sort[0][column]=period", 
        "sort[0][direction]=desc", 
        paste0("offset=", the_offset), 
        paste0("length=", the_length),
        sep = "&"
      ))
  
      eia_list <- fromJSON(str_c(get_call, "&api_key=", the_eia_key))
      
      eia_data <- eia_list$response$data
      
      eia_data %>% 
        as_tibble() %>% 
        return()
      }
    
    else{
      
      eia_list <- fromJSON(str_c(api_url, "&api_key=", the_eia_key))
    
      eia_data <- eia_list$response$data
      
      eia_data %>% 
        as_tibble() %>% 
        return()
    
    }
  }

```

```{r}

url.wind <- "https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=monthly&data[0]=generation&facets[fueltypeid][]=AOR&facets[location][]=TX&facets[sectorid][]=99&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000"

data.wind.gen <- fn_query_eia(api_url = url.wind)

data.price <- fn_query_eia(the_series_id = "ELWHU_TX", the_source = "steo", the_facet = "seriesId")

data.wind.cf <- fn_query_eia(api_url = "https://api.eia.gov/v2/total-energy/data/?frequency=monthly&data[0]=value&facets[msn][]=WYCASUS&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000")

```

The data is split into two datasets, a training and testing dataset. The testing set are the most recent 12 months, while the training set are the 48 months preceding that. 

```{r}




data <- left_join(
  x = data.price %>% 
    select(period, value) %>% 
    rename(date = period, price = value) %>% 
    mutate(date = ym(date) %>% yearmonth()),
  y = data.wind.gen %>% 
    select(period, generation) %>% 
    rename(date = period) %>% 
    mutate(date = ym(date) %>% yearmonth()),
  by = "date"
) %>% 
  left_join(
    y = data.wind.cf %>% 
    select(period, value) %>% 
    rename(date = period, capacity.factor = value) %>% 
    mutate(date = ym(date) %>% yearmonth()),
  by = "date"
  ) %>% 
  arrange(date) %>% 
  mutate(
    price_lag12 = lag(price, n = 12),
    price_lag18 = lag(price, n = 18),
    price_lag24 = lag(price, n = 24),
    price_lag30 = lag(price, n = 30),
    price_lag36 = lag(price, n = 36),
    price_lag48 = lag(price, n = 48),
    price_lag60 = lag(price, n = 60),
    price_lag72 = lag(price, n = 72),
    price_lag12_ma = rollmean(price, k = 12, fill = NA, align = "right") %>% lag(n = 12),
    price_lag24_ma = rollmean(price, k = 24, fill = NA, align = "right") %>% lag(n = 24),
    price_lag36_ma = rollmean(price, k = 36, fill = NA, align = "right") %>% lag(n = 36),
    price_lag48_ma = rollmean(price, k = 48, fill = NA, align = "right") %>% lag(n = 48)
  ) %>% 
  drop_na()

avg.capacity <- data %>% 
  mutate(month = month(date)) %>% 
  group_by(month) %>% 
  summarize(capacity.factor = mean(capacity.factor)) %>% 
  ungroup()

test <- data %>% 
  select(-capacity.factor) %>% 
  slice_max(order_by = date, n = 12) %>% 
  mutate(date = yearmonth(date), month = month(date)) %>% 
  left_join(avg.capacity, by = "month") %>% 
  select(-month) %>% 
  tsibble()

train <- data %>% 
  slice_max(order_by = date, n = 12*10) %>% 
  mutate(date = yearmonth(date)) %>% 
  anti_join(y = test, by = "date") %>% 
  tsibble()

data <- data %>% 
  tsibble(index = date)

```


## Preliminary Analysis


### Data Exploration

#### Monthly Wind Generation in Texas

```{r}
#| fig-height: 4

train %>% 
  gg_tsdisplay(generation)

```

#### Monthly Average Wholesale Electricity Price in Texas (log scale)

```{r}
#| fig-height: 4

train %>% 
  gg_tsdisplay(log(price))

```

#### Decomposition

The STL Decomposition breaks out the data into the trend, season, and remainder components. The seasonality will largely be driven by wind though I would suspect repairs in the shoulder months playing a role. The trend will show the general increase in supply over time. The remainder will contain wind deviations from normal, unexpected turbine outages, and other unforeseeable factors.

```{r}

train %>% 
  model(STL(generation)) %>% 
  components() %>% 
  autoplot() +
  scale_y_continuous(labels = label_comma())

```


#### Correlations

The 30-month lag appears to be the most correlated.

```{r}

train %>% 
  pivot_longer(-c(date, generation, capacity.factor), names_to = "lag", values_to = "value") %>% 
  ggplot(aes(
    x = log(value),
    # x = value,
    y = box_cox(generation, .208),
    # y = generation,
    color = capacity.factor
  )) +
  geom_point() +
  geom_smooth(se = F, scales = "free", span = 2, color = "gray30") +
  facet_wrap(lag ~ .) +
  scale_color_viridis_c() +
  theme(
    legend.position = "bottom"
  )

```


## Modeling

#### Estimation

Four models will be estimated: an ETS, an auto-ARIMA, and an ARIMAX using gas and diesel prices as regressors. The fourth model will be a simple average of the others. 

```{r}


(fit <- train %>% 
  model(
    "ets" = ETS(box_cox(generation, .208)),
    "arima" = ARIMA(box_cox(generation, .208) ~ log(price_lag12_ma) + log(price_lag24_ma) + log(capacity.factor)),
    "nn" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 3, scale_inputs = TRUE
    ),
    "nn2" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor) + 
        price_lag30, # + price_lag24, #+
        # AR(P = 1),
      n_nodes = 5, scale_inputs = TRUE
    )
  )) #%>% 
   # mutate(ensemble = (ets + arima + nn) / 3))


```

```{r}

fit %>% 
  select(arima) %>% 
  report()

```


## Forecast

The models were trained on data prior to `r max(as.Date(train$date)) + months(1)`. The forecast period is the interval `r lubridate::interval(min(as.Date(test$date)), max(as.Date(test$date)))`. Three forecasts are produced from each model, one from each of the three scenarios (low, medium, and high).


```{r}

fx <- fit %>% 
    forecast(test, times = 100)
  

fx %>% 
  autoplot(
      # data %>% filter(year(date)>2020),
      level = NULL, size = .75, alpha = .75
    ) +
  autolayer(
    data %>% tsibble() %>% filter(year(date)>=2020), generation, 
    size = 1, alpha = .75#, linetype = "dashed"
  ) +
  ggtitle("Texas Wind Generation Out-of-Sample Forecast") + 
  # scale_color_viridis_d() +
  scale_y_continuous(labels = label_comma()); fx %>% 
  accuracy(test, measures = point_accuracy_measures) %>% 
  arrange(RMSE)

```

The ETS model performs best by a large margin with an RMSE of $8,674. 

```{r}

fx %>% 
  accuracy(test, measures = list(point_accuracy_measures, distribution_accuracy_measures)) %>% 
  arrange(RMSE)

```

## Cross Validation

```{r}

train.cv <- train %>% 
  stretch_tsibble(.init = 12, .step = 3)

train.cv$.id %>% max()

```


```{r}

(fit.cv <- train.cv %>% 
  model(
    "nn2" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 2, scale_inputs = TRUE
    ),
     "nn3" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 3, scale_inputs = TRUE
    ),
     "nn4" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 4, scale_inputs = TRUE
    ),
     "nn5" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 5, scale_inputs = TRUE
    ),
     "nn8" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 8, scale_inputs = TRUE
    ),
     "nn15" = NNETAR(
      box_cox(generation, .208) ~ log(price_lag24_ma) + log(price_lag36_ma) + (capacity.factor), #+ 
        # price_lag18 + price_lag24 + 
        # AR(P = 1),
      n_nodes = 15, scale_inputs = TRUE
    )
    )
  ) #%>% 
   # mutate(ensemble = (ets + arima + nn) / 3))

```

```{r}

fit.cv %>% 
  forecast(train.cv, times = 100) %>% 
  accuracy(train.cv)

```


## Bagged Forecasts

Due to the uncertainty in the regressors, bagging the forecast could aid tremendously. We'll generate 100 new training sets by simulating an STL model. We'll create a new test set that combines them into one and updates the tsibble key to include these scenarios. 

### Simulating New Data

```{r}


sales_stl <- train %>% 
  model(STL(sales.gas))

sim <- sales_stl %>% 
  generate(new_data = train, times = 100, bootstrap_block_size = 12) %>% 
  select(-.model, -sales.gas)

test.sim <- list(
    test.high %>% rename(.sim = sales.gas) %>% mutate(.scenario = "high") %>% as_tibble(),
    test.med %>% rename(.sim = sales.gas) %>% mutate(.scenario = "medium") %>% as_tibble(),
    test.low %>% rename(.sim = sales.gas) %>% mutate(.scenario = "low") %>% as_tibble()
  ) %>% 
  do.call(bind_rows, .) %>% 
  cross_join(y = tibble(.rep = as.character(1:100))) %>% 
  tsibble(key = c(.rep, .scenario))

```


Using this simulated training set, we build new models of the same type just as before. Each model type will be fit to each of the 20 training sets in turn giving $100*3=300$ different models. These models will then forecast the testing data for each scenario, resulting in $300*3=900$ different forecasts.

### Model & Forecast

The below code estimated these models and saved the output for this report compilation to read.

```{r}
#| eval: false

sim_models <- sim %>% 
  model(
    ets = ETS(.sim),
    arima = ARIMA(.sim ~ pdq(d = 1) + PDQ(D=0)),
    dynamic = ARIMA(.sim ~ gas.price + pdq(d = 1) + PDQ(D=0)),
    # dynamic2 = ARIMA(.sim ~ gas.price + diesel.price + pdq(d = 1) + PDQ(D=0))
  )

sim_forecasts <- lapply(test.sim %>% split(~.scenario), \(x){
  sim_models %>% 
    forecast(update_tsibble(x, key = .rep))
}) %>% 
  lapply(as_tibble) %>% 
  do.call(bind_rows, .) %>% 
  tsibble(key = c(.scenario, .rep, .model))


saveRDS(sim_models, "sim_models.RDS")
saveRDS(sim_forecasts, "sim_forecasts.RDS")

```


```{r}

sim_models <- readRDS("sim_models.RDS") %>% 
  select(-dynamic2)


sim_forecasts <- readRDS("sim_forecasts.RDS")


sim_forecasts %>% 
  autoplot(.mean) +
  autolayer(
    data %>% tsibble() %>% filter(year(date)>2020), sales.gas
  ) +
  guides(colour = "none") +
  labs(title = "Gasoline Station Sales: bootstrapped forecasts on low, medium, and high scenarios.")

```

These bootstrapped forecasts are then averaged together to create a new ensemble forecast. The below plot compares this bagged forecast against the best scenario, the high scenario. The dashed lines are the bagged forecast at the 80th percentile, mean, and 20th percentile, respectively.

```{r}

bagged <- sim_forecasts %>% 
  summarise(
    bagged_mean = mean(.mean),
    bagged_median = median(.mean),
    bagged_p80 = quantile(.mean, .8),
    bagged_p20 = quantile(.mean, .2)
  )

fx.high %>% 
  autoplot(
      data %>% tsibble() %>% filter(year(date)>2020), 
      level = NULL,
      size = .75, alpha = .75#, linetype = "dashed"
  ) +
  autolayer(bagged, bagged_mean, col = "gray30", size = 1, linetype = "dashed") +
  autolayer(bagged, bagged_p80, col = "gray30", size = 1, linetype = "dashed") +
  autolayer(bagged, bagged_p20, col = "gray30", size = 1, linetype = "dashed") +
  labs(title = "Gasoline Station Sales: bootstrapped forecasts")

```


## Model Comparison

The following table summarizes each of these models and scenarios:

```{r}

bind_rows(
    fx.low %>% mutate(.scenario = "low") %>% as_tibble() %>% select(.scenario, .model, date, .mean),
    fx.med %>% mutate(.scenario = "med") %>% as_tibble() %>% select(.scenario, .model, date, .mean),
    fx.high %>% mutate(.scenario = "high") %>% as_tibble() %>% select(.scenario, .model, date, .mean),
    bagged %>% 
      as_tibble() %>% 
      select(date, bagged_mean) %>% 
      pivot_longer(-date, names_to = ".model", values_to = ".mean") %>% 
      mutate(.scenario = "bagged")
  ) %>% 
  left_join(test, by = "date") %>% 
  as_tibble() %>% 
  group_by(.scenario, .model) %>% 
  summarize(
    RMSE = RMSE(.mean - sales.gas),
    MAPE = MAPE(.mean - sales.gas, .actual = sales.gas),
    MAE = MAE(.mean - sales.gas)
  ) %>% 
  arrange(RMSE)

```

The bagged forecast outperforms all the other forecasts, even the dynamic model in the high scenario. This is despite the high scenario dynamic model only containing the *better* data in the test set and the bagged forecast containing all low, medium, and high scenarios. A better forecast could possibly also be achieved by simulating the test set gas and diesel prices rather than using scenario forecasts. This approach would also enable us to quantify the uncertainty present in the exogenous regressors better. The first approach produces near meaningless confidence intervals, since those do not consider the uncertainty in the regressors. Simulating these variables and calculating the quantiles enables us to embed this uncertainty within the forecast. 