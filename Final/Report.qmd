---
title: "Dengue Fever Forecasting"
author: "Shaun Harrington"
format: html
  code-fold: true
  fig-height: 4
  fig-width: 8
execute:
  warning: false
  error: false
editor_options: 
  chunk_output_type: console
---

```{r setup}
#|eval: true
#|echo: false

require(tidyverse)
require(fpp3)
library(modeltime)

if(!stringr::str_detect(basename(getwd()), "Time Series") & stringr::str_detect(dirname(getwd()), "Time Series")){
    repeat{
      setwd("../")
      if(stringr::str_detect(basename(getwd()), "Time Series")){
        break
      }
    }
  }
  
  if(basename(getwd()) != "Final") setwd(file.path(getwd(), "Final"))

source(file.path("00_Setup.R"))

# 
(all_cores <- parallel::detectCores())
(cl <- makePSOCKcluster(all_cores))
registerDoParallel(cl)

```

## Introduction

#### Background

<!-- Brief about Dengue fever, its global and local significance. -->
<!-- Importance of forecasting for Dengue fever cases. -->

#### Objective

<!-- Clearly state the aim of the paper, i.e., to forecast Dengue fever cases in the two cities using predictive analytics. -->

## Methodology

### Data Description & Exploration

<!-- Description of the dataset: sources, time period, variables, etc. -->
<!-- Discuss any data cleaning or preprocessing steps. -->

The dataset is provided by Driven Data where a training set and test set are given. The training dataset not only contains weekly Dengue Fever cases but various weather variables including precipitation, humidity, various temperatures, vegetation growth, and annual population levels for each city. Several other variables are created from these existing exogenous variables. 

* Growing Degree Days (GDD): GDDs are used to approximate mosquito population growth rates as they are positively correlated. A baseline of $10$

pmin(pmax(station_avg_temp_c - 10, 0), 34-10)

#### Time Plots

```{r}

train %>% 
  autoplot(total_cases)

```

```{r}

train.all %>% 
  autoplot(case_rate + .000001) +
  scale_y_continuous(labels = label_percent(), trans = "logit")

train %>% 
  autoplot(box_cox(case_rate, -.15))

train %>% 
  autoplot((total_cases + 1)/population) +
  scale_y_continuous(labels = label_percent(), trans = "log10")

```


#### Seasonality

```{r}

train %>% 
  gg_season(total_cases) 

```


#### Decomposition

```{r}

train %>% 
  model(STL(log(total_cases+1))) %>% 
  components() %>% 
  autoplot()

```



```{r}

L = 5
train %>%
  mutate(
    cases_365d_cut = cut_number(cases_365d, n = L),
    precip_4w_cut = cut_number(precip_4w_sa, n = L),
    humidity_rel_avg_4w_cut = cut_number(humidity_rel_avg_4w, n = 4) %>% lag(n = L),
    humidity_rel_avg_2w_cut = cut_number(humidity_rel_avg_2w, n = L),
    hdd_reanalysis_4w_cut = cut_number(hdd_reanalysis_4w_sa, n = L),
    hdd_station_4w_cut = cut_number(hdd_station_4w, n = L)
  ) %>% drop_na() %>% 
  ggplot(aes(
    x = lag(precip_4w_sa, n = L), 
    y = lag(hdd_station_4w_sa, n = L), 
    color = total_cases_scaled, alpha = total_cases_scaled
  )) +
  geom_point() +
  facet_grid(city ~ humidity_rel_avg_4w_cut, scales = "free", drop = T) +
  scale_color_viridis_c() +
  xlab("Rolling 4-week Precipitation, Seasonally Adjusted with 5 week lag") +
  ylab("Rolling 4-week Heating Degree Days, Seasonally Adjusted with 5 week lag") +
  ggtitle(
    "Interaction of Precipitation and Heating Degree Days Influencing Dengue Fever Cases Across Humidity Intervals"
  ) +
  theme(
    legend.position = "bottom"
  )


```



### Modeling

#### Description of Models

<!-- Discuss the rationale for selecting these models. -->
<!-- Provide a brief theoretical background for each model. -->

#### Modeling Training

<!-- Description of the training set and validation set. -->

<!-- Discuss any hyperparameter tuning or model optimization steps.
Discuss any hyperparameter tuning or model optimization steps. -->


##### Model 1: Mean

####### Iquitos



```{r m1_fit_iq}
#| eval: false
#| echo: true

fit1.iq <- train %>% 
  filter(city == "iq") %>% 
  filter(cases_cumulative >= 10) %>% 
  model(
    fit1 = TSLM(box_cox(cases_cumulative, .8) ~ fourier(K = 10) + trend())
  )

```



```{r m1_fit_iq_save}
#| eval: false
#| echo: false

saveRDS(fit1.iq, "Report Objects/fit1.iq.RDS")


```


```{r m1_fit_iq_read}
#| eval: true
#| echo: false

fit1.iq <- readRDS("Report Objects/fit1.iq.RDS")


```



####### San Jose 

Estimation

```{r m1_fit_sj}

fit1.sj <- train %>% 
  filter(city == "sj") %>% 
  filter(cases_cumulative >= 10) %>% 
  model(
    fit1 = TSLM(box_cox(cases_cumulative, 2.15) ~ fourier(K = 10) + trend())
  )


```



```{r m1_fit_sj_save}
#| eval: false
#| echo: false

saveRDS(fit1.sj, "Report Objects/fit1.sj.RDS")


```


```{r m1_fit_sj_read}
#| eval: true
#| echo: false

fit1.sj <- readRDS("Report Objects/fit1.sj.RDS")


```





##### Model 2: SARIMAX

####### Iquitos



```{r m2_fit_iq}
#| eval: false

fit2.iq <- train %>% 
  filter(city == "iq") %>% 
  filter(cases_cumulative > 10) %>% 
  model(
    fit2 = ARIMA(
      box_cox(cases_cumulative, .7) ~ 1 + 
        fourier(K = 5) + PDQ(D=0,Q=0) +
        lag(precip_4w, n = 10) +
        # rollmean(reanalysis_tdtr_k_sa, k = 5, fill = NA, align = "right") +
        lag(hdd_reanalysis_4w_sa * humidity_rel_avg_4w_sa, n = 6)
      # + PC15 %>% lag(n = 6)
      # + PC20 %>% lag(n = 1)
      # + PC1 %>% lag(n = 1)
      + lag(station_diur_temp_rng_c, n = 5)
      + lag(hdd_reanalysis_365d_sa, n = 5)
      + lag(hdd_station_365d, n = 10)
      + lag(precip_365d_sa, n = 8)
    )
  )

```



```{r m2_fit_iq_save}
#| eval: false
#| echo: false

saveRDS(fit2.iq, "Report Objects/fit2.iq.RDS")


```


```{r m2_fit_iq_read}
#| eval: true
#| echo: false

fit2.iq <- readRDS("Report Objects/fit2.iq.RDS")


```




####### San Jose



```{r m2_fit_sj}
#| eval: false


fit2.sj <- train %>% 
  filter(city == "sj") %>% 
  filter(cases_cumulative > 10) %>% 
  model(
    fit2 = ARIMA(
      box_cox(cases_cumulative, .7) ~ 1 + 
        fourier(K = 5) + PDQ(D=0,Q=0) +
        lag(precip_4w, n = 10) +
        rollmean(reanalysis_tdtr_k_sa, k = 5, fill = NA, align = "right") +
        lag(hdd_reanalysis_4w_sa * humidity_rel_avg_4w_sa, n = 6)
      # + PC15 %>% lag(n = 6)
      # + PC20 %>% lag(n = 1)
      # + PC1 %>% lag(n = 1)
      + lag(station_diur_temp_rng_c, n = 5)
      + lag(hdd_reanalysis_365d_sa, n = 5)
      + lag(hdd_station_365d, n = 10)
      + lag(precip_365d_sa, n = 8)
    )
  )

```



```{r m2_fit_sj_save}
#| eval: false
#| echo: false

saveRDS(fit2.sj, "Report Objects/fit2.sj.RDS")


```


```{r m2_fit_sj_read}
#| eval: true
#| echo: false

fit2.sj <- readRDS("Report Objects/fit2.sj.RDS")


```


##### Model 3: Prophet with Boosted Errors

####### Iquitos


```{r m3_recipe_iq}

recipe_spec_iq <- train %>%
  as_tibble() %>% 
  recipe(total_cases ~ ., data = .) %>%
  step_filter(city == "iq") %>% 
  step_timeseries_signature(week_start_date) %>% 
  step_rm(
    any_of(setdiff(!!vars.y, "total_cases"))
  ) %>%
  step_rm(
    any_of(setdiff(!!vars.id, c("weekofyear", "week_start_date")))
  ) %>% 
  step_rm(contains("PC")) %>% 
  step_rm(
    contains("am.pm"), contains("hour"), contains("minute"),
    contains("second"), contains("xts"), contains("day"), contains("susc")
  ) %>% 
  step_fourier(week_start_date, period = 52, K = 5) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_lag(all_numeric_predictors(), lag = 1:10) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_diff(contains("_sa")) %>% 
  step_naomit(all_predictors(), total_cases) %>% 
  step_log(total_cases, offset = 1)

```


```{r m3_recipe_view_iq}
#| echo: false
#| eval: false

recipe_spec_iq %>% prep() %>% juice() %>% #select(week_start_date, contains("cases")) #%>% View()
  colnames() %>% sort()

recipe_spec_iq$var_info #%>% View()


```


```{r m3_spec_iq}

model_spec_iq <- prophet_boost(mtry = tune(), tree_depth = tune(), learn_rate = tune(), min_n = tune()) %>%
  set_engine("prophet_xgboost", yearly.seasonality = TRUE, weekly.seasonality = TRUE) 

workflow_fit_proph_boost_iq <- workflow() %>% 
  add_model(model_spec_iq) %>% 
  add_recipe(recipe_spec_iq)

```


```{r m3_tuning_iq}
#| eval: false

(tuning.grid_iq <- grid_regular(
  mtry(c(200, 500)),
  learn_rate(c(-2, -.5)),
  tree_depth(c(40, 100)),
  min_n(c(20,30)),
  levels = c(6, 3, 5, 4)
))

(folds_iq <- rolling_origin(
  train %>% filter(city == "iq"), 
  cumulative = F,
  initial = 52*4, assess = 50, skip = 60
))

# Tune model
fit3.tuning_iq <- workflow_fit_proph_boost_iq %>%
  tune_grid(folds_iq, grid = tuning.grid_iq)

```


```{r m3_tuning_iq_save}
#| eval: false
#| echo: false

# collect_notes(fit3.tuning_iq)$note[1]
# show_notes(.Last.tune.result)
saveRDS(fit3.tuning_iq, "Report Objects/fit3.tuning_iq.RDS")
```


```{r m3_tuning_iq_read}
#| eval: true
#| echo: false

fit3.tuning_iq <- readRDS("Report Objects/fit3.tuning_iq.RDS")

```


```{r m3_cv_results_line_iq}
#| eval: false
#| echo: false

collect_metrics(fit3.tuning_iq) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(
    x = learn_rate,
    y = mean,
    color = as.factor(mtry)
  )) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymax = mean + std_err, ymin = mean - std_err), alpha = .25) +
  scale_x_log10() +
  facet_grid(tree_depth ~ min_n, scales = "free_x") +
  scale_y_continuous(labels = label_comma()) +
  theme_bw() +
  theme(legend.position = "bottom")


```


```{r m3_cv_results_raster_iq}
#| fig-height: 6

collect_metrics(fit3.tuning_iq) %>% 
  filter(.metric == "rmse") %>% 
  rename(RMSE = mean) %>% 
  ggplot(aes(
    x = learn_rate,
    y = mtry, 
    fill = RMSE
  )) +
  geom_raster(interpolate = T) + 
  scale_x_log10() +
  facet_grid(tree_depth ~ min_n, scales = "free_x") +
  scale_y_continuous(labels = label_comma()) +
  scale_fill_viridis_c(option = "B", direction = -1) +
  ggtitle("Heat Map of Cross Validated RMSE facted by tree depth (horizontally) and min_n (vertically)") +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(2, "cm"))

```


```{r m3_final_iq}
#| eval: false
#| echo: true

(best.param_iq <- select_best(fit3.tuning_iq, "rmse"))

wf.final_iq <- workflow_fit_proph_boost_iq %>% 
  finalize_workflow(best.param_iq)

fit3.iq <- wf.final_iq %>% 
  fit(train)
    
```



```{r m3_fit_iq_save}
#| eval: false
#| echo: false

saveRDS(fit3.iq, "Report Objects/fit3.iq.RDS")


```


```{r m3_fit_iq_read}
#| eval: true
#| echo: false

fit3.iq <- readRDS("Report Objects/fit3.iq.RDS")


```





####### San Jose



```{r m3_recipe_sj}

recipe_spec_sj <- train %>%
  as_tibble() %>%
  filter(city == "sj") %>% 
  recipe(case_rate ~ ., data = .) %>%
  step_timeseries_signature(week_start_date) %>% 
  step_rm(
    any_of(setdiff(!!vars.y, "case_rate"))
  ) %>%
  step_rm(
    any_of(setdiff(!!vars.id, c("weekofyear", "week_start_date")))
  ) %>% 
  step_rm(contains("PC")) %>% 
  step_rm(
    contains("am.pm"), contains("hour"), contains("minute"),
    contains("second"), contains("xts"), contains("day"), contains("susc")
  ) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_lag(all_numeric_predictors(), lag = 1:10) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_diff(contains("_sa")) %>% 
  step_naomit(all_predictors(), case_rate) %>% 
  step_logit(case_rate, offset = .00001)


```


```{r m3_recipe_view_sj}
#| echo: false
#| eval: false

recipe_spec_sj %>% prep() %>% juice() %>% #select(week_start_date, contains("cases")) #%>% View()
  colnames() %>% sort()

recipe_spec_sj$var_info #%>% View()


```



```{r m3_spec_sj}

model_spec_sj <- prophet_boost(mtry = tune(), tree_depth = tune(), learn_rate = tune(), min_n = tune()) %>%
  set_engine("prophet_xgboost", yearly.seasonality = TRUE, weekly.seasonality = TRUE) 

workflow_fit_proph_boost_sj <- workflow() %>% 
  add_model(model_spec_sj) %>% 
  add_recipe(recipe_spec_sj)

```


```{r m3_tuning_sj}
#| eval: false

(tuning.grid_sj <- grid_regular(
    mtry(c(50, 150)),
    learn_rate(c(-1, 0)),
    tree_depth(c(20, 60)),
    min_n(c(5,20)),
    # levels = c(2,2,1,1)
    levels = c(3, 4, 3, 3)
  ))

(folds_sj <- rolling_origin(
  train %>% filter(city == "sj"), 
  cumulative = F,
  initial = 52*5, assess = 100, skip = 100
))

fit3.tuning_sj <- workflow_fit_proph_boost_sj %>%
  tune_grid(folds_sj, grid = tuning.grid_sj)

```

```{r m3_tuning_sj_save}
#| echo: false
#| eval: false

# collect_notes(fit3.tuning_sj)$note[1]
# show_notes(.Last.tune.result)

saveRDS(fit3.tuning_sj, "Report Objects/fit3.tuning_sj.RDS")

```

```{r m3_tuning_sj_read}
#| eval: true
#| echo: false

fit3.tuning_sj <- readRDS("Report Objects/fit3.tuning_sj.RDS")

```

```{r m3_cv_results_line_sj}
#| eval: false
#| echo: false

collect_metrics(fit3.tuning_sj) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(
    x = learn_rate,
    y = mean,
    color = as.factor(mtry)
  )) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymax = mean + std_err, ymin = mean - std_err), alpha = .25) +
  scale_x_log10() +
  facet_grid(tree_depth ~ min_n, scales = "free_x") +
  scale_y_continuous(labels = label_comma()) +
  theme_bw() +
  theme(legend.position = "bottom")


```


```{r m3_cv_results_raster_sj}
#| fig-height: 6

collect_metrics(fit3.tuning_sj) %>% 
  filter(.metric == "rmse") %>% 
  rename(RMSE = mean) %>% 
  ggplot(aes(
    x = learn_rate,
    y = min_n, 
    fill = RMSE
  )) +
  geom_raster(interpolate = T) + 
  scale_x_log10() +
  facet_grid(tree_depth ~ mtry, scales = "free_x") +
  scale_y_continuous(labels = label_comma()) +
  scale_fill_viridis_c(option = "B", direction = -1) +
  ggtitle("Heat Map of Cross Validated RMSE facted by tree depth (horizontally) and m_try (vertically)") +
  theme_bw() +
  theme(legend.position = "bottom", legend.key.width = unit(2, "cm"))

```


```{r m3_final_sj}


(best.param_sj <- select_best(fit3.tuning_sj, "rmse"))


wf.final_sj <- workflow_fit_proph_boost_sj %>% 
  finalize_workflow(best.param_sj)


fit3.sj <- wf.final_sj %>% 
  fit(train)
    
```



```{r m3_fit_sj_save}
#| eval: false
#| echo: false

saveRDS(fit3.sj, "Report Objects/fit3.sj.RDS")


```


```{r m3_fit_sj_read}
#| eval: true
#| echo: false

fit3.sj <- readRDS("Report Objects/fit3.sj.RDS")


```






#### Model Evaluation

<!-- Metrics used to evaluate the model (e.g., MAE, RMSE, etc.). -->
<!-- Discuss the validation process. -->





## Results

<!-- Model Performance on Training Set. Provide tables or charts illustrating model performance. -->

<!-- Forecast on the Test Set -->
<!-- Display the forecasts of each model on the test data. -->
<!-- Compare the performance of the models using the test data. -->

#### Model Performance on Training Set

##### Iquitos

###### Model 1


```{r}
#| fig-height: 3

fit1.iq %>% 
  gg_tsresiduals(lag_max = 100)

```




###### Model 2


```{r}
#| fig-height: 4

fit2.iq %>% 
  gg_tsresiduals(lag_max = 100)

```



###### Model 3

```{r m3_var_imp_iq}


fit3.orig.iq <- extract_fit_engine(fit3.iq)

xgboost::xgb.importance(model = fit3.orig.iq$models$model_2) %>%
  xgboost::xgb.plot.importance(top_n = 20) 

```



##### San Jose

###### Model 1

```{r}

fit1.sj %>% 
  gg_tsresiduals(lag_max = 100)

```




###### Model 2


```{r}

fit2.sj %>% 
  gg_tsresiduals(lag_max = 100)

```





###### Model 3



```{r m3_var_imp_sj}


fit3.orig.sj <- extract_fit_engine(fit3.sj)

xgboost::xgb.importance(model = fit3.orig.sj$models$model_2) %>%
  xgboost::xgb.plot.importance(top_n = 20) 

```




#### Model Performance on Validation Set

##### Iquitos


```{r fx_valid_iq}

fx1.iq <- fn_standardize_fx(forecast(fit1.iq, valid), the_model = 1, the_city = "iq")
fx2.iq <- fn_standardize_fx(forecast(fit2.iq, valid), the_model = 2, the_city = "iq")

calibration_table_iq <- modeltime_table(fit3.iq) %>%
  modeltime_calibrate(valid)

fx3.iq_trans <- calibration_table_iq %>%
  modeltime_forecast(
    # new_data = as_tibble(train.all) %>% filter(city == "iq", year >= 2008)
    new_data = as_tibble(valid) %>% filter(city == "iq")
  )


fx3.iq <- fn_standardize_fx(fx3.iq_trans, the_model = 3, the_city = "iq", the_smooth = 7)

```



###### Ensemble


```{r fx_valid_avg_iq}

fx.iq <- bind_rows(
  fx1.iq,
  fx2.iq,
  fx3.iq
)

fx.iq_avg <- fx.iq %>% 
  as_tibble() %>% 
  group_by(city, yearweek) %>% 
  summarize(across(contains("predicted"), \(x){mean(x, na.rm = T)})) %>% 
  arrange(yearweek) %>% 
  mutate(.model = "ensemble") %>% 
  tsibble(index = yearweek, key = c(city, .model))

fx.iq <- bind_rows(fx.iq, fx.iq_avg)


```

```{r}
#| echo: false
#| eval: false

saveRDS(fx.iq, "Report Objects/fx_valid.iq.RDS")

```



###### Comparison

```{r fx_valid_plot_iq}

fx.iq %>% 
  autoplot(predicted_cases) + 
  autolayer(
    valid %>% filter(city == "iq"), 
    total_cases
  )

```


```{r fx_valid_mae_iq}

(valid.mae_iq <- fx.iq %>% 
  left_join(y = valid %>% select(city, yearweek, total_cases)) %>% 
  as_tibble() %>% 
  group_by(.model) %>% 
  summarize(
    MAE = MAE(predicted_cases - total_cases)
  ))


```



##### San Jose



```{r fx_valid_sj}

fx1.sj <- fn_standardize_fx(forecast(fit1.sj, valid), the_model = 1, the_city = "sj")
fx2.sj <- fn_standardize_fx(forecast(fit2.sj, valid), the_model = 2, the_city = "sj")

calibration_table_sj <- modeltime_table(fit3.sj) %>%
  modeltime_calibrate(valid %>% filter(city == "sj"))

fx3.sj_trans <- calibration_table_sj %>%
  modeltime_forecast(
    # new_data = as_tibble(train.all) %>% filter(city == "sj", year >= year(ymd(valid.start.sj)))
    new_data = as_tibble(valid) %>% filter(city == "sj")
  )

fx3.sj <- fn_standardize_fx(
  fx3.sj_trans, the_model = 3, the_city = "sj", the_smooth = 10
)

```



###### Ensemble


```{r fx_valid_avg_sj}

fx.sj <- bind_rows(
  fx1.sj,
  fx2.sj,
  fx3.sj
)

fx.sj_avg <- fx.sj %>% 
  as_tibble() %>% 
  group_by(city, yearweek) %>% 
  summarize(across(contains("predicted"), \(x){mean(x, na.rm = T)})) %>% 
  arrange(yearweek) %>% 
  mutate(.model = "ensemble") %>% 
  tsibble(index = yearweek, key = c(city, .model))

fx.sj <- bind_rows(fx.sj, fx.sj_avg)


```


```{r}
#| echo: false
#| eval: false

saveRDS(fx.sj, "Report Objects/fx_valid.sj.RDS")

```

###### Comparison

```{r fx_valid_plot_sj}

fx.sj %>% 
  autoplot(predicted_cases) + 
  autolayer(
    valid %>% filter(city == "sj"), 
    total_cases
  )

```


```{r fx_valid_mae_sj}

(valid.mae_sj <- fx.sj %>% 
  left_join(y = valid %>% select(city, yearweek, total_cases) %>% filter(city == "sj")) %>% 
  as_tibble() %>% 
  group_by(.model) %>% 
  summarize(
    MAE = MAE(predicted_cases - total_cases)
  ))


```


#### Final Fit Models on Full Training Set

```{r fit_final}

fit1.iq_final <- fit1.iq %>% 
  refit(train.all %>% filter(city == "iq"))

fit2.iq_final <- fit2.iq %>% 
  refit(train.all %>% filter(city == "iq"))

fit3.iq_final <- wf.final_iq %>% 
  fit(train.all %>% filter(city == "iq"))


fit1.sj_final <- fit1.sj %>% 
  refit(train.all %>% filter(city == "sj"))

fit2.sj_final <- fit2.sj %>% 
  refit(train.all %>% filter(city == "sj"))

fit3.sj_final <- wf.final_sj %>% 
  fit(train.all %>% filter(city == "sj"))

```

#### Forecast on Test Set


##### Iquitos

```{r fx_test_iq}

fx1.test.iq <- fit1.iq_final %>% 
  forecast(test) %>% 
  fn_standardize_fx(., 1, "iq")

fx2.test.iq <- fit2.iq_final %>% 
  forecast(test) %>% 
  fn_standardize_fx(., 2, "iq")

fx3.test.iq <- modeltime_table(fit3.iq_final) %>%
  modeltime_calibrate(test) %>%
  modeltime_forecast(
    new_data = as_tibble(test) %>% filter(city == "iq")
  ) %>% 
  fn_standardize_fx(., 3, "iq", 7)


fx.test.iq <- bind_rows(fx1.test.iq, fx2.test.iq, fx3.test.iq)

fx.test.iq_avg <- fx.test.iq %>% 
  as_tibble() %>% 
  group_by(city, yearweek) %>% 
  summarize(across(contains("predicted"), \(x){mean(x, na.rm = T)})) %>% 
  arrange(yearweek) %>% 
  mutate(.model = "ensemble") %>% 
  tsibble(index = yearweek, key = c(city, .model))

fx.test.iq <- bind_rows(fx.test.iq, fx.test.iq_avg)


```


##### San Jose


```{r fx_test_sj}

fx1.test.sj <- fit1.sj_final %>% 
  forecast(test) %>% 
  fn_standardize_fx(., 1, "sj")

fx2.test.sj <- fit2.sj_final %>% 
  forecast(test) %>% 
  fn_standardize_fx(., 2, "sj")

fx3.test.sj <- modeltime_table(fit3.sj_final) %>%
  modeltime_calibrate(test) %>%
  modeltime_forecast(
    new_data = as_tibble(test) %>% filter(city == "sj")
  ) %>% 
  fn_standardize_fx(., 3, "sj", 7)


fx.test.sj <- bind_rows(fx1.test.sj, fx2.test.sj, fx3.test.sj)

fx.test.sj_avg <- fx.test.sj %>% 
  as_tibble() %>% 
  group_by(city, yearweek) %>% 
  summarize(across(contains("predicted"), \(x){mean(x, na.rm = T)})) %>% 
  arrange(yearweek) %>% 
  mutate(.model = "ensemble") %>% 
  tsibble(index = yearweek, key = c(city, .model))

fx.test.sj <- bind_rows(fx.test.sj, fx.test.sj_avg)

```


```{r fx_test_save}
#| eval: false
#| echo: false

bind_rows(fx.test.iq, fx.test.sj) %>% 
  saveRDS(., "Report Objects/fx_test.RDS")

fx.test_list <- bind_rows(fx.test.iq, fx.test.sj) %>% 
  # left_join(y = test %>% select(city, yearweek, year, weekofyear)) %>% 
  mutate(
    year = year(yearweek),
    # weekofyear = week(yearweek)
    weekofyear = (as.Date(yearweek) + days(1)) %>% week
  ) %>%
  rename(total_cases = predicted_cases) %>% 
  as_tibble() %>%
  # filter(yearweek >= yearweek(ymd(test.start.sj))) %>% 
  select(.model, city, yearweek, year, weekofyear, total_cases) %>%
  split(~.model) 

x <- fx.test_list[[1]]
fx.test_list %>%
  lapply(., \(x){
    the_model <- x$.model[1]
    
    sub_format <- read_csv(file.path("Data/submission_format.csv")) %>% 
      select(-total_cases)
    
    submission <- x %>% 
      as_tibble() %>% 
      select(-.model) %>% 
      mutate(
        city = factor(city, levels = c("sj", "iq"))
      ) %>% 
      arrange(city, yearweek) 
    
    submission %>% 
      right_join(y = sub_format) %>% 
      select(city, year, weekofyear, total_cases) %>% 
      replace_na(list(total_cases = 2)) %>% 
      write_csv(., file.path("Data/Test Forecast", paste0("Submission_", the_model, ".csv")))
  })



```



```{r fx_test_plot}

bind_rows(
  fx.test.iq, fx.test.sj, 
  train.all %>% select(city, yearweek, total_cases)
) %>% 
  as_tibble() %>% 
  filter(year(yearweek) >= 2003) %>% 
  pivot_longer(c(predicted_cases, total_cases), names_to = "series", values_to = "cases") %>% 
  filter(!is.na(cases)) %>% 
  ggplot(aes(x = yearweek, y = cases, color = interaction(.model, series))) +
  geom_line() +
  facet_grid(city ~ ., scales = "free")

```

## Discussion

<!-- Model Comparisons: Discuss the strengths and weaknesses of each model in the context of the study. -->

<!-- Implications: Potential impact of accurate forecasts on healthcare planning and interventions. -->


<!-- Limitations: Any limitations of your study, be it in data, methodology, or the models. -->


<!-- Future Work: Suggestions for further research or improvement in the forecasting models. -->


## Conclusion

<!-- Summary of Key Findings -->

<!-- Final Thoughts on the Potential of Predictive Analytics in Forecasting Dengue Cases. -->





```{r}
#| echo: false
parallel::stopCluster(cl)
```

